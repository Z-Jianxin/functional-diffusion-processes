# @package _global_
defaults:
  - override /trainers: trainer_vit
  - override /models: uvit
  - override /datasets: celeba
  - override /samplers: ode_sampler
  - override /sdes: ode

trainers:
  mode: "eval"
  model_name: "350001_f95famvk:v0" # "local" # [MODIFIED]
  training_config:
    save_dir: ${oc.env:LOGS_ROOT}/train_uvit_celeba_frf_prednoise350000_eval_euler10 # [MODIFIED]
    ema_rate: 0.9999
    weight_decay: 0.03
    sampling_only: True # [MODIFIED]
  evaluation_config:
    seed: 43 # random seed for reproducibility
    eval_dir: ${oc.env:LOGS_ROOT}/train_uvit_celeba_frf_prednoise_350000_eval_euler10 # [MODIFIED]
    num_samples: 50000 # number of samples to be generated for evaluation [ATTENTION] # training size: "162770"
  trainer_logging:
    use_wandb: True
    wandb_init:
      name: ${trainers.training_config.save_dir} # [NEW]
      project: "frf_tune_vit_celeba" # [NEW]
  optimizer:
    _target_: "optax.MultiSteps"
    opt:
      _target_: "optax.chain"
      _args_:
        # - _target_: "optax.clip"
        #  max_delta: 1.0
        - _target_: "optax.adamw"
          learning_rate:
            _target_: "optax.warmup_cosine_decay_schedule"
            init_value: 0.0
            peak_value: ${trainers.training_config.peak_value}
            warmup_steps: ${trainers.training_config.warmup_steps}
            decay_steps: ${trainers.training_config.decay_steps}
            end_value: ${trainers.training_config.end_value}
          weight_decay: ${trainers.training_config.weight_decay}
    every_k_schedule: ${trainers.training_config.gradient_accumulation_steps}

correctors:
  snr: 0.16

samplers:
  sampler_config:
    N: 10 # [MODIFIED]
    k: 1
    denoise: True
    rtol: 1e-1 #[NEW] parameter for adaptive solver
    atol: 5e-2 #[NEW] parameter for adaptive solver
    t0: 0 # [NEW] starting time for sampling
    t1: 0.999 # [NEW] end time for sampling
    clip: True # [NEW]
    clip_lower: -1.0 # [NEW]
    clip_upper: 1.0 # [NEW]
    solver: "Euler" #[NEW] Euler or Dopri5
    dt0: 0.02 #[NEW]

sdes:
  sde_config:
    beta_max: 10.0
    const: 0.02
    probability_flow: True
    factor: 0.5
    x_norm: 64
    energy_norm: 64
    psm_choice: "fdp" # [NEW] type of psm matrix to use
    prior_type: "fdp" # [NEW] type of noise prior
    predict_noise: True # [NEW] predict noise, Esser et al. 2024


models:
  model_config:
    patch_size: 1
    embeddings_size: 512
    add_position: "encoding"
    is_unidimensional: False
    transformer:
        num_heads: 8
        num_layers: 7
        mlp_dim: 2048
        mlp_ratio: 4
        dropout_rate: 0.0
        attention_dropout_rate: 0.0
        skip: True



datasets:
  test:
    data_config:
      batch_size: 16
